{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-project\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General guidelines\n",
    "\n",
    "We want to make a typical study of a ML problem.\n",
    "\n",
    "We're going to use Fashion-MNIST (\"*fashion-mnist-reshaped.npz*\")  as data set, attempting to classify the pictures correctly.\n",
    "\n",
    "There are 2 parts in the project:\n",
    "- use `DecisionTreeClassifier` and PCA from sklearn to classify the data\n",
    "- make your own multi-class classifier, deriving its updates from scratch\n",
    "The first part weights more in the total grade than the second one.\n",
    "\n",
    "In the first part, the goal is to showcase a typical hyper-parameter tuning. We will simulate the fact of having different tasks by restricting ourselves to different dataset size, and comment on how hyper-parameters choice can depend a lot on how much data we have at hand.\n",
    "\n",
    "General advice: **write clean code**, well factored in functions/classes, for each question, as much as possible.\n",
    "This will make your code **easier to read and also easier to run!**. You may re-use code in several questions. If it's  well factored, it will be easier to code the next questions.\n",
    "\n",
    "Tips: you may want to use \n",
    "- `sklearn.tree.DecisionTreeClassifier`\n",
    "- `sklearn.model_selection.train_test_split`\n",
    "- `sklearn.decomposition.PCA`\n",
    "- `sklearn.model_selection.cross_validate` \n",
    "\n",
    "to lighten your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: using `sklearn.tree.DecisionTreeClassifier`\n",
    "\n",
    "## (about 15 points over 20 total)\n",
    "\n",
    "Decision Trees are powerful methods, however they can easily overfit. The number of parameters in the model essenitially grows like $\\sim O(2^{maxDepth})$, i.e. exponentially with the depth of the tree.\n",
    "\n",
    "### Part 1.1: `Ntrain+Nval=2000, Nvalid=1000`\n",
    "\n",
    "In this part we use this amount of data.\n",
    "- import the data, split the \"train+validation\" sets. Keep the test set for the **very** end.\n",
    "- attempt direct classification using a `sklearn.tree.DecisionTreeClassifier`. Optimize the hyper-parameter `max_depth`. Measure and store the validation accuracy for the best choice of `max_depth`.\n",
    "Do you fear you may be overfitting ? Explain your answer.\n",
    "- Now, let's add some PCA as pre-processing. \n",
    "    - Using `max_depth=5`, what is the best number of PCA components (nComp_PCA) to keep ? Hint: you may use something like `nComp_range = np.array(list(np.arange(1,50))+[50,100,200,400,783,784])` as the range of nComp_PCA values to be explored.\n",
    "    - Using `max_depth=12`, what is the best number of PCA components (nComp_PCA) to keep ?\n",
    "    - Can you explain why this optimal number changes with depth ? \n",
    "- Find the best (max_depth, nComp_PCA) pair. \n",
    "- Can you explain the behavior of the optimal `max_depth`, let's call it $m*$, with `nComp_PCA`, at **small** `nComp_PCA` ?\n",
    "- Can you explain the behavior of the optimal `max_depth`, let's call it $m*$, with `nComp_PCA`, at **large** `nComp_PCA` ?\n",
    "- Measure the cross-validation error for this best pair. Are you surprised with the result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LoadObject = np.load(\"../../data/fashion-mnist-reshaped.npz\") # please put your data over there so it's easy for me to run your code\n",
    "linearPictureLength = 28\n",
    "X = LoadObject['train_images']\n",
    "y = LoadObject['train_labels']\n",
    "## we do not use the TEST SET for now:\n",
    "# Xtest = LoadObject['test_images']\n",
    "# ytest = LoadObject['test_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.2: `Ntrain+Nval=5000, Nvalid=2000`\n",
    "\n",
    "If you factored your code decently in the last questions, this should be very easy/fast to do. Ideally, it should be a couple of lines and a single function call. (For the core computation, excluding plots and presentation)\n",
    "- split the \"train+validation\" sets. \n",
    "- Find the best (max_depth, nComp_PCA) pair. \n",
    "- Measure the cross-validation error for this best pair. Are you surprised with the result?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.3: `Ntrain+Nval=20000, Nvalid=10000`\n",
    "\n",
    "If you factored your code decently in the last questions, this should be very easy/fast to do. Ideally, it should be a couple of lines and a single function call. (For the core computation, excluding plots and presentation)\n",
    "- split the \"train+validation\" sets.\n",
    "- Find the best (max_depth, nComp_PCA) pair. \n",
    "- Measure the cross-validation error for this best pair. Are you surprised with the result?\n",
    "\n",
    "**Hint: to save compute time, you can use a smaller hyper-parameter search space, i.e. you can reduce the umber of values explored in your hyper-optimization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.4: The test (with `Ntest=10000`)\n",
    "Use your best model to make a prediction:\n",
    "- Which model do you prefer, among the 3 \"best models\" you have found? Why? How confident are you with your choice?\n",
    "- Using your `Ntest=10000` samples that you saved preciously (and NEVER used), compute the test error. How surprised are you with the result? \n",
    "- If you were asked by a client, \"what is the level of accuracy you can achieve\", what would be your answer ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xtest = LoadObject['test_images']\n",
    "# ytest = LoadObject['test_labels']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.4 - Bonus question : \n",
    "- Compute also the cross validation error for the best hyper parameters choice with `N_train=200`\n",
    "- Plot the cross validation error as a function of ntrain= 200,2000,20000  \n",
    "- People often say \"let's just get more data\". How efficient does that does seem to be ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: make your own classifier !\n",
    "\n",
    "## (about 5 points over 20 total)\n",
    "\n",
    "The multi-class percpetron can be implemented this way. \n",
    "We denote $K$ the number of classes, $N$ the number of (training) examples, $D$ the dimension of the data (after feature augmentation, at least with a \"1\" as first component).\n",
    "\n",
    "The **output** of the network *(not equal to the predicted label)*, can be taken as the **softmax** among the $K$ separating hyperplanes (each hyperplane $\\vec{w}_k$ separates class $k$ from the others).\n",
    "$$ y_k^{(n)} = \\text{softmax}\\big( (\\vec{w}_{k} \\cdot \\vec{x}^{(n)})_{k=1...K} \\big) = \\frac{ \\exp(  \\vec{w}_k\\cdot\\vec{x}^{(n)}   )}{\\sum_\\ell \\exp(  \\vec{w}_\\ell\\cdot\\vec{x}^{(n)})}$$\n",
    "This output can be **interpreted as the probability** that example $x^{(n)}$ belongs to the class $k$, according the classifier's current parameters\n",
    "Indeed, one can easily check that for any $\\vec{x}$, the sum of probabilities is indeed one : $\\sum_k y_k = 1$.\n",
    "The **total output of the network** is a vector $\\vec{y}^{(n)} = \\begin{pmatrix}y_1^{(n)} \\\\ y_2^{(n)} \\\\ .. \\\\ y_K^{(n)} \\end{pmatrix}$ (for the sample number $n$).\n",
    "\n",
    "The **true labels (ground truth)** of example $\\vec{x}^{(n)}$ is then encoded as a one-hot vector, so that if the example is of the second class, it may be written: $\\vec{t}^{n} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ .. \\\\ 0 \\end{pmatrix}$. (where $\\vec{t}^{(n)}$ or $\\vec{t}^{n}$ is for **T**ruth and is shorter to write than $\\vec{y}^{GT,(n)}$). More generally, the components $t_{n,k}$ of vector $\\vec{t}_n$ may be written using the Kronecker's delta: $t_{n,k} = \\delta(k, k_{true}^n)$, where $k_{true}^n$ is the true class of example number $n$.\n",
    "\n",
    "From now on, **we drop the superscrip $a^{(n)}$ and instead write $a_n$ or just $a$**, when it's clear enough that the quantity $a$ relates to a single example, of generic index $n$. This helps to lighten the notations.\n",
    "\n",
    "The Loss function that we should use is called the **cross-entropy loss function**, and is:\n",
    "\n",
    "$$J = \\frac1N \\sum_n^N H(\\vec{t}_{n}, \\vec{y}_{n})$$\n",
    "\n",
    "where the cross-entropy is a non-symmetric function: $$H(\\vec{t}_{n}, \\vec{y}_{n}) = -\\sum_k^K t_{n,k} \\log (y_{n,k})$$ \n",
    "\n",
    "Make sure you undersand all of the above. Write down the Loss function for the multi-class perceptron. \n",
    "### Part 2.1\n",
    "- What are the parameters of the model ? **How many real numbers is that ?** Count them in terms of $N,K,D, etc$. \n",
    "- (3-4 points) **Derive the update steps for the gradient**. (you can get inspiration from TD4.1)\n",
    "- Some Hints:\n",
    "    - It is recommended to compute the quantity $\\nabla_{w_\\ell} y_k$ ($\\ell\\neq k$) and the quantity $\\nabla_{w_k} y_k$. Try to express these simply, by recognizing $y$ when it appears. First treat the two cases separately, then try to unite the two cases in a single mathematical form, using Kronecker's delta : $\\delta(i,j)= \\{1$ if $i=j$, else $0\\}$.\n",
    "    - When there is a sum $\\sum_\\ell f(w_\\ell)$ and you derive with respect to $w_k$, the output only depends on the term $f(w_k)$ \n",
    "    - In the sum above, $\\sum_\\ell f(w_\\ell)$ the index $\\ell$ is a \"mute\" index: you can use any letter for it. Be careful not to use a letter that already exists outside the sum ($\\ell$ is like a local variable, don't use the same name for a \"global variable\" from outside the function !)\n",
    "    - For any functions $u,v$ that admit derivatives, $\\partial_x \\frac{u(x)}{v(x)} = \\frac{u'(x)v(x)-u(x)v'(x)}{(v(x))^2}$. It extends to $\\nabla_x$ without problem.\n",
    "    - $\\nabla_x \\exp(u(x)) =  \\exp(u(x)) \\nabla_x u(x)$.\n",
    "    - $\\frac{a}{1+a} = 1- \\frac{1}{1+a}$\n",
    "    - $\\partial_x \\log(u(x)) = \\frac{u'(x)}{u(x)}$ \n",
    "    - If you are too much blocked, you can ask me (via discord, in Private Message) for the solution of $\\nabla_{w_k} y_k$ and/or the solution for $\\nabla_{w_\\ell} y_k$ ($\\ell\\neq k$).\n",
    "    - In the end, the update step for the parameters that you should find is : $$ \\vec{w}_\\ell \\mapsto \\vec{w}_\\ell + \\eta \\frac1N \\sum_n^N \\vec{x}_n (\\delta_{\\ell, k_{true}^n}- y_{\\ell,n})$$\n",
    "    - If you cannot find the equation above, you can just skip this question and use it to make your program.\n",
    "    \n",
    "    \n",
    "\n",
    "### Part 2.2\n",
    "- (3 points) **Think up of all the functions you need to write**, and **put them in a class** (you can get inspiration from the correction of TP3.2) - first write a class skeleton, and **only then, write the methods** inside\n",
    "- Hints:\n",
    "    - there may be numerical errors (NaNs) because $\\exp(..)$ is too large. You can ease this by noticing the following: for any positive constant $C$, we have $$\\frac{ \\exp( a_k  )}{\\sum_\\ell \\exp (a_\\ell) }  = \\frac{C \\exp( a_k  )}{C \\sum_\\ell \\exp (a_\\ell) }= \\frac{\\exp( a_k +\\log C )}{\\sum_\\ell \\exp (a_\\ell +\\log C) }$$\n",
    "    - with this trick, when your arguments in the softmax are too large, you can simply subtract a big constant $\\log C$ from its argument, and this will reduce the chances of numerical error, without changing the result. It's a good idea to change the $w $'s with this kind of trick.\n",
    "    - it's a good idea to define the target labels (ground truth) data in one-hot vectors (as said above), compute them once and for all, and then you never have to compute them again. In practice, you may notice that for an example with label $k_{true}$, then the genreic component number of $k$ of the vector $\\vec{t}$ reads: $t_{k} = \\delta_{k, k_{true}}$\n",
    "    - the initial $w$ should be random (not all zeros), preferably, but not too big. A good idea is to have their dispersion be of order $1/D$ at most.\n",
    "    \n",
    "For this question, the main goal is to make a theoretically-working, rather clean code, using numpy array-operations (`np.dot`) and not loops, as much as possible. If you manage to do that, you will most likely have a working code (and fast code!)\n",
    "- (1 point) Test your algorithm on Fashion-MNIST: make a train / validation / test split , fit the model, compute the cross-val error, and the test error. Don't waste time on optimizing hyper-parameters (just take an $\\eta$ small enough that you kind of converge. The goal is really to prove that your algorithm does not always crash :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6881171418161356e+43\n"
     ]
    }
   ],
   "source": [
    "## remark:\n",
    "import numpy as np\n",
    "print(np.exp(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flandes/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: RuntimeWarning: overflow encountered in exp\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/flandes/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(np.exp(800)- np.exp(800))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
